# THE BLIND OCTOPUS EXPERIMENT

## Project Overview
This project implements a Multi-Agent Reinforcement Learning (MARL) system inspired by the decentralized control seen in octopuses. The system simulates an octopus-like environment where each agent controls a different limb of a shared body. The agents learn to collaborate and achieve specific goals, such as reaching food, using reinforcement learning algorithms like Deep Q-Networks (DQN).

## File Descriptions

1. **main.py**
   - This file serves as the main entry point for training and evaluating the MARL system.
   - It sets up the environment (`OctopusEnv`), initializes the agents, and trains the DQN-based agents.
   - The file also contains evaluation logic to compare the performance of the learned agent against a random agent baseline.

2. **octopus_env.py**
   - This file defines the environment `OctopusEnv` in which the octopus-like agents operate.
   - It simulates the shared body, tracks the agents' states, and handles the reward structure, making it the core of the multi-agent environment.
   
3. **octopus.py**
   - This file defines the `Octopus` class, which implements the multi-agent DQN model.
   - It includes the architecture of the neural network, the action selection mechanism for the agents, and the training process (with experience replay and target networks).
   - The class handles how each agent independently controls part of the body while learning to cooperate.

4. **random_agent.py**
   - Defines a simple `RandomAgent` class that selects actions randomly for each agent.
   - This is used as a baseline for comparison with the learned DQN agents in the evaluation process.

5. **multi_agent_dqn.pth**
   - This file contains the saved model weights of a pre-trained multi-agent DQN model.
   - It can be loaded to evaluate the agents' learned policy without re-training.

6. **test_env_02.py**
   - This script provides additional evaluation functionality, including replay data saving.
   - It loads the trained DQN model and evaluates its performance on various grid sizes and energy levels, comparing it with the random agent.
   - The script also supports saving replay data in a JSON format for further analysis or visualization.

## How to Use

1. **Training**: 
   - Run `main.py` to train the DQN agents in the octopus environment. The script also evaluates the agents periodically to track progress.
   
2. **Evaluation**: 
   - Use `test_env_02.py` to evaluate the pre-trained model (`multi_agent_dqn.pth`) or other saved models. It compares the DQN agents' performance with a random agent and generates replay data if needed.

3. **Replay Data**:
   - Replay data generated by `test_env_02.py` is saved in JSON format and can be analyzed for insights into agent behavior and coordination.

## Requirements
- Python 3.x
- PyTorch
- NumPy
- Matplotlib (for plotting results)

## Contact Information
For any questions, feel free to contact Simone Esposito at simone.esposito11@studio.unibo.it
